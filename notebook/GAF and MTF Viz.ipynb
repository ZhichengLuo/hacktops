{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/bdmagr4/ayobi/.local/lib/python3.6/site-packages/numba/core/errors.py:154: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#hacktop package\n",
    "from hacktops.data import generate_top_dataset\n",
    "from hacktops.data import get_well_relevant_windows\n",
    "\n",
    "# pyts\n",
    "import pyts\n",
    "from pyts.image import MarkovTransitionField\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining necessary functions\n",
    "setting_seed = 432\n",
    "\n",
    "def random_sample_n(X, k):\n",
    "    samples = random.sample(X, k=k)\n",
    "    return samples\n",
    "\n",
    "def data_viz(input_x, input_y, status):\n",
    "    random.seed(setting_seed)\n",
    "    true_samples = [input_x[i] for i in range(len(input_x)) if input_y[i]==status]\n",
    "    samples = random_sample_n(true_samples, 32)\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16,4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        data = samples[i]\n",
    "        ax.plot(data)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show\n",
    "\n",
    "# function to plot Gramian Angular images \n",
    "\n",
    "def gaf_viz(input_x, input_y, status):\n",
    "    random.seed(setting_seed)\n",
    "    true_samples = [input_x[i] for i in range(len(input_x)) if input_y[i]==status]\n",
    "    samples = random_sample_n(true_samples, 32)\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16,4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        data = samples[i]\n",
    "        ax.imshow(data)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show\n",
    "\n",
    "\n",
    "#1. Function to plot model's validation loss and validation accuracy\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# fit and evaluate a model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #add model layers\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='sigmoid', input_shape=data_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# defining confusion matrix plot\n",
    "def plot_matrix(matrix):\n",
    "    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in matrix.flatten()/np.sum(matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plot = sns.heatmap(matrix, annot=labels, fmt=\"\", cmap='Blues')\n",
    "    return plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn\n",
    "from tslearn.metrics import dtw as tslearn_dtw\n",
    "import pandas as pd\n",
    "import hacktops\n",
    "import numpy as np\n",
    "def instance_norm(sample: np.array):\n",
    "    s = (sample - np.min(sample)) / (np.max(sample) - np.min(sample) + 1)\n",
    "    return s\n",
    "from pyspark import SparkContext\n",
    "from dtaidistance.dtw import distance as dtai_dtw\n",
    "from dtaidistance.dtw import distance_fast as dtai_dtw_fast\n",
    "from hacktops.settings import WINDOW_LENGTH\n",
    "from tslearn.metrics import dtw as tslearn_dtw\n",
    "from pyts.metrics import dtw as pyts_dtw\n",
    "import random\n",
    "from plotly.offline import iplot\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from hacktops.data import generate_top_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TopFinder:\n",
    "    \"\"\"\n",
    "    TopFinder: wrapper for window classifier\n",
    "    \n",
    "    Limitations:\n",
    "    - Work on single one top and assume independence among tops\n",
    "    - Find top by classifying windows extracted from well data and discard\n",
    "      the correlation between windows\n",
    "    - Does not utilize geographical info of wells\n",
    "\n",
    "    Usage example:\n",
    "\n",
    "        >>> model.fit(dataset)\n",
    "        >>> model.evaluate_windows = a_func\n",
    "\n",
    "        >>> top_finder = TopFinder(model, top_name)\n",
    "        >>> top_finder.examine_dataset(df_tops)\n",
    "\n",
    "        >>> predicted_depth = top_finder.find_top(df_well)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fitted_window_classifier, top_name):\n",
    "        if fitted_window_classifier.evaluate_windows is None:\n",
    "            raise ValueError(\"fitted_window_classifier has to have function evaluate_windows\")\n",
    "        self.window_classifier = fitted_window_classifier\n",
    "        self.work_on_top = top_name\n",
    "        self.stats = {}\n",
    "\n",
    "    def examine_dataset(self, df_tops:pd.DataFrame):\n",
    "        self.stats['top_depth_max'] = df_tops[self.work_on_top].max()\n",
    "        self.stats['top_depth_min'] = df_tops[self.work_on_top].min()\n",
    "\n",
    "    def extract_window(self, df_well:pd.DataFrame, center_idx, window_length):\n",
    "        left_limit = center_idx - window_length\n",
    "        right_limit = center_idx + window_length\n",
    "        window = df_well.loc[left_limit : right_limit, 'GR'].to_numpy()\n",
    "        return window\n",
    "\n",
    "    def get_candidate_windows(self, df_well:pd.DataFrame):\n",
    "        '''\n",
    "        extra prior knowledge may be used to narrow down the scope of candidates, \n",
    "        e.g. top distribution. \n",
    "\n",
    "        return list of windows. Each window includes the depth of its center & GR data.\n",
    "        '''\n",
    "        max_, min_ = self.stats['top_depth_max'], self.stats['top_depth_min']\n",
    "        center_  = (max_ + min_) / 2\n",
    "        depth_diff_ = max_ - min_\n",
    "        dilated_max_ =  center_ + DILATION_RATIO * depth_diff_ / 2\n",
    "        dilated_min_ =  center_ - DILATION_RATIO * depth_diff_ / 2\n",
    "\n",
    "        windows = []\n",
    "        for idx, row in df_well.iterrows():\n",
    "            if row['DEPTH'] < dilated_max_ and row['DEPTH'] > dilated_min_:\n",
    "                window_depth = row['DEPTH']\n",
    "                window_data = self.extract_window(df_well, idx, WINDOW_LENGTH)\n",
    "                if window_data.shape != (WINDOW_LENGTH * 2 + 1,):\n",
    "                    # print(window_data.shape) \n",
    "                    # It happens when the window gets out of the scope of well depth\n",
    "                    continue\n",
    "                windows.append((window_depth, window_data))\n",
    "        return windows\n",
    "\n",
    "    def select_window(self, windows, scores: np.array):\n",
    "        '''\n",
    "        extra prior knowledge may be used here, e.g. top relationships\n",
    "        '''\n",
    "        index_max = np.argmax(scores, axis=0)\n",
    "        return windows[index_max]\n",
    "\n",
    "    def find_top(self, df_well):\n",
    "        \"\"\"\n",
    "        Step:\n",
    "            1. Extract all candidate windows from the well\n",
    "            2. Evalute each candidate by window classifier\n",
    "            3. Select the best candidate\n",
    "            4. Return its associated depth\n",
    "        \"\"\"\n",
    "        if self.window_classifier is None:\n",
    "            raise Exception(\"window_classifier is not set\")\n",
    "        if df_well.shape[0] == 0:\n",
    "            raise Exception(\"input well has no data\")\n",
    "\n",
    "        self.windows = self.get_candidate_windows(df_well)\n",
    "        print(f'{len(self.windows)} candidate windows')\n",
    "        windows_data = np.array([w[1] for w in self.windows])\n",
    "        self.scores = self.window_classifier.evaluate_windows(windows_data)\n",
    "        selected_window = self.select_window(self.windows, self.scores)\n",
    "        self.top_depth = selected_window[0]\n",
    "\n",
    "        return self.top_depth\n",
    "\n",
    "\n",
    "class SimpleDTWWindowEvaluator:\n",
    "    \"\"\"\n",
    "    Simple DTW Window Evaluator\n",
    "\n",
    "    Fit: Caches a set of real top windows\n",
    "\n",
    "    Evaluate: Scores = [AVG(1 / 1 + dtw(candidate, real)) for each candidate]\n",
    "    \"\"\"\n",
    "    def __init__(self, metric=tslearn_dtw, norm=instance_norm):\n",
    "        self.metric = metric\n",
    "        self.norm = norm\n",
    "\n",
    "    def fit(self, real_top_windows):\n",
    "        real_top_windows = np.array([self.norm(w) for w in real_top_windows])\n",
    "        self._real_top_windows = real_top_windows\n",
    "\n",
    "    def evaluate_windows(self, candidate_windows) -> np.array:\n",
    "        scores = []\n",
    "        candidate_windows = [self.norm(w) for w in candidate_windows]\n",
    "        for w in tqdm(candidate_windows):\n",
    "            dists = np.array([self.metric(_w, w) for _w in self._real_top_windows])\n",
    "            weights = 1 / (1 + dists)\n",
    "            scores.append(weights.sum() / weights.shape[0])\n",
    "        scores = np.array(scores)\n",
    "        return scores\n",
    "\n",
    "class SimpleDTWWindowEvaluator_Spark:\n",
    "    \"\"\"\n",
    "    Spark Version of Simple DTW Window Evaluator\n",
    "    \"\"\"\n",
    "    def __init__(self, sc:SparkContext, metric=tslearn_dtw, norm=instance_norm):\n",
    "        self.metric = metric\n",
    "        self.sc = sc\n",
    "        self.norm = norm\n",
    "\n",
    "    def fit(self, real_top_windows):\n",
    "        real_top_windows = np.array([self.norm(w) for w in real_top_windows])\n",
    "        self._real_top_windows_rdd = self.sc.parallelize(real_top_windows)\n",
    "\n",
    "    def evaluate_windows(self, candidate_windows):\n",
    "        candidate_windows = [self.norm(w) for w in candidate_windows]\n",
    "        input_rdd = self.sc.parallelize([(i, candidate_windows[i]) for i in range(len(candidate_windows))])\n",
    "        aTuple = (0,0)\n",
    "        metric = self.metric\n",
    "        scores = input_rdd.cartesian(self._real_top_windows_rdd)\\\n",
    "                .map(lambda t : (t[0][0], 1 / (1 + metric(t[0][1], t[1]))))\\\n",
    "                .aggregateByKey(aTuple, lambda a,b: (a[0] + b,    a[1] + 1),\n",
    "                                        lambda a,b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                .mapValues(lambda v: v[0] / v[1])\\\n",
    "                .sortByKey().map(lambda t : t[1])\\\n",
    "                .collect()\n",
    "        return scores\n",
    "\n",
    "# class NewModel:\n",
    "#     def ...\n",
    "#\n",
    "#     def evaluate_windows(self, candidate_windows):\n",
    "#         ...\n",
    "#         return scores\n",
    "\n",
    "def get_true_windows(df_logs, df_tops, top_, keep_depth = False):\n",
    "    dataset = generate_top_dataset(df_logs=df_logs, df_tops=df_tops, top=top_)\n",
    "    all_well_names = df_logs['wellName'].unique()\n",
    "    print(f'{len(dataset[0])} windows extracted from {len(all_well_names)} wells')\n",
    "\n",
    "    X = np.array(dataset[0]).squeeze(axis=2)\n",
    "    y = np.array(dataset[1])\n",
    "    \n",
    "    print('X:', X.shape)\n",
    "    print('y:', y.shape)\n",
    "\n",
    "    true_idx = [idx for idx in range(len(X)) if y[idx] == True]\n",
    "    print(f'{len(true_idx)} true windows left')\n",
    "\n",
    "    return X[true_idx]\n",
    "\n",
    "def get_true_depth(wellname, top, df_tops):\n",
    "    return df_tops.loc[wellname, top]\n",
    "\n",
    "def visual_scores(depths, scores, max_score_depth=None, true_depth=None, well_name=None):\n",
    "    data = []\n",
    "    data.append(go.Scatter(x=depths,y=scores))\n",
    "    title = \"Evaluation Score w.r.t depth\"\n",
    "    if well_name:\n",
    "        title += f' [well: {well_name}]'\n",
    "    fig = go.Figure(data=data, layout={'title':title})\n",
    "    if max_score_depth:\n",
    "        fig.add_vline(x=max_score_depth, line_width=2, line_color=\"yellow\", \\\n",
    "            annotation_text='Predicated', annotation_position='top left')\n",
    "    if true_depth:\n",
    "        fig.add_vline(x=true_depth, line_width=2, line_color=\"green\", \\\n",
    "            annotation_text='True', annotation_position='top right')\n",
    "    return fig\n",
    "\n",
    "def get_true_windows(df_logs, df_tops, top_, keep_depth = False):\n",
    "    dataset = generate_top_dataset(df_logs=df_logs, df_tops=df_tops, top=top_)\n",
    "    all_well_names = df_logs['wellName'].unique()\n",
    "    print(f'{len(dataset[0])} windows extracted from {len(all_well_names)} wells')\n",
    "\n",
    "    X = np.array(dataset[0]).squeeze(axis=2)\n",
    "    y = np.array(dataset[1])\n",
    "    \n",
    "    print('X:', X.shape)\n",
    "    print('y:', y.shape)\n",
    "\n",
    "    true_idx = [idx for idx in range(len(X)) if y[idx] == True]\n",
    "    print(f'{len(true_idx)} true windows left')\n",
    "\n",
    "    return X[true_idx]\n",
    "\n",
    "def get_true_depth(wellname, top, df_tops):\n",
    "    return df_tops.loc[wellname, top]\n",
    "\n",
    "\n",
    "def visual_scores(depths, scores, max_score_depth=None, true_depth=None, well_name=None):\n",
    "    data = []\n",
    "    data.append(go.Scatter(x=depths,y=scores))\n",
    "    title = \"Evaluation Score w.r.t depth\"\n",
    "    if well_name:\n",
    "        title += f' [well: {well_name}]'\n",
    "    fig = go.Figure(data=data, layout={'title':title})\n",
    "    if max_score_depth:\n",
    "        fig.add_vline(x=max_score_depth, line_width=2, line_color=\"yellow\", \\\n",
    "            annotation_text='Predicated', annotation_position='top left')\n",
    "    if true_depth:\n",
    "        fig.add_vline(x=true_depth, line_width=2, line_color=\"green\", \\\n",
    "            annotation_text='True', annotation_position='top right')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ = 'MARCEL'\n",
    "df_logs_ = pd.read_parquet(\"../data/logs.parquet\")\n",
    "df_loc_ = pd.read_parquet(\"../data/loc.parquet\")\n",
    "df_tops_ = pd.read_parquet(\"../data/tops.parquet\")\n",
    "\n",
    "df_logs_test_ = pd.read_parquet(\"../testdata/logs_50.parquet\")\n",
    "df_loc_test_ = pd.read_parquet(\"../testdata/loc_50.parquet\")\n",
    "df_tops_test_ = pd.read_csv(\"../testdata/tops_50.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAN FOUND\n"
     ]
    }
   ],
   "source": [
    "train_dataset = generate_top_dataset(df_logs=df_logs_, df_tops=df_tops_, top=top_)\n",
    "test_dataset = generate_top_dataset(df_logs=df_logs_test_, df_tops=df_tops_test_, top=top_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (119800, 61) (119800,)\n",
      "Testing set:  (10000, 61) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_dataset[0]).reshape(len(train_dataset[0]), len(test_dataset[0][0]))\n",
    "y_train = np.array(train_dataset[1])\n",
    "X_test = np.array(test_dataset[0]).reshape(len(test_dataset[0]), len(test_dataset[0][0]))\n",
    "y_test = np.array(test_dataset[1])\n",
    "\n",
    "print(\"Training set: \", X_train.shape,y_train.shape)\n",
    "print(\"Testing set: \", X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size_gaf = 40\n",
    "gasf = GramianAngularField(image_size=image_size_gaf, method= 'summation')\n",
    "gadf = GramianAngularField(image_size=image_size_gaf, method= 'difference')\n",
    "mtf = MarkovTransitionField(image_size=image_size_gaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset: converting train dataset to images\n",
    "train_gasf_images = gasf.fit_transform(X_train)\n",
    "train_gadf_images = gadf.fit_transform(X_train)\n",
    "train_mtf_images = mtf.transform(X_train)\n",
    "\n",
    "# test dataset: converting test dataset to images\n",
    "test_gasf_images = gasf.fit_transform(X_test)\n",
    "test_gadf_images = gadf.fit_transform(X_test)\n",
    "test_mtf_images = mtf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gasf_images.shape, train_gadf_images.shape, train_mtf_images.shape, test_gasf_images.shape, test_gadf_images.shape, test_mtf_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing training data\n",
    "data_viz(X_train, y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing test data\n",
    "data_viz(X_test, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_viz(X_test, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "gaf_viz(train_gasf_images, y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "gaf_viz(test_gasf_images, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf_viz(train_gadf_images, y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf_viz(test_gadf_images, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf_viz(train_mtf_images, y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf_viz(test_mtf_images, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gasf_data = np.expand_dims(train_gasf_images, axis=3)\n",
    "train_gadf_data = np.expand_dims(train_gadf_images, axis=3)\n",
    "train_mtf_data = np.expand_dims(train_mtf_images, axis=3)\n",
    "train_combined = np.concatenate((np.expand_dims(train_gasf_images, axis=3),np.expand_dims(train_gadf_images, axis=3), np.expand_dims(train_mtf_images, axis=3)), axis=3)\n",
    "\n",
    "test_gasf_data = np.expand_dims(test_gasf_images, axis=3)\n",
    "test_gadf_data = np.expand_dims(test_gadf_images, axis=3)\n",
    "test_mtf_data = np.expand_dims(test_mtf_images, axis=3)\n",
    "test_combined = np.concatenate((np.expand_dims(test_gasf_images, axis=3),np.expand_dims(test_gadf_images, axis=3), np.expand_dims(test_mtf_images, axis=3)), axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GASF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data\n",
    "x_input = train_gasf_data\n",
    "\n",
    "# selecting model shape\n",
    "data_shape = x_input.shape[1:]\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_history = model.fit(x_input, y_train, epochs=15, batch_size=32, validation_split=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f\"{top_}_gasf_model.h5\")\n",
    "#plot_model_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test data\n",
    "model.evaluate(test_gasf_data, y_test)\n",
    "y_predict = model.predict(test_gasf_data)\n",
    "y_predict=np.argmax(y_predict,axis=1)\n",
    "\n",
    "cf_matrix = confusion_matrix(y_predict,y_test)\n",
    "\n",
    "print(classification_report(y_predict,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_windows(self, candidate_windows):\n",
    "    candidate_windows = gasf.fit_transform(candidate_windows)\n",
    "    candidate_windows.reshape(len(candidate_windows), 1)\n",
    "    return self.evaluate(candidate_windows)\n",
    "import types\n",
    "model.evaluate_windows = types.MethodType(evaluate_windows, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_finder = TopFinder(model, top_)\n",
    "top_finder.examine_dataset(df_tops_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_well_names = df_logs_test_['wellName'].unique()\n",
    "print(len(test_well_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for test_well_name in tqdm(test_well_names):\n",
    "    print(f'well: {test_well_name}')\n",
    "    df_test_well = df_logs_test_[df_logs_test_['wellName'] == test_well_name]\n",
    "    predicted_depth = top_finder.find_top(df_test_well)\n",
    "    true_depth = get_true_depth(test_well_name, top_, df_tops_test_)\n",
    "    result.append([test_well_name, predicted_depth, true_depth])\n",
    "    # print(f'true depth: {true_depth}\\t predicated depth: {predicted_depth}\\t error: {abs(predicted_depth - true_depth)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(result, columns=['wellName', 'predicated_depth', 'true_depth']).set_index('wellName')\n",
    "df_tops_pred = df_result[['predicated_depth']].rename(columns={'predicated_depth': top_})\n",
    "df_tops_true = df_result[['true_depth']].rename(columns={'true_depth': top_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hacktops.evaluate import recall_tops\n",
    "\n",
    "recall, mae, df_res = recall_tops(df_tops_true, df_tops_pred, tolerance = 4)\n",
    "print(\"recall {0}, mae {1}\".format(recall,mae))\n",
    "df_res.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [w[0] for w in top_finder.windows]\n",
    "fig = visual_scores(depth, top_finder.scores, top_finder.top_depth, true_depth, test_well_name)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data\n",
    "x_input = x_gadf_data\n",
    "\n",
    "# split in to test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, Y, test_size=0.30, random_state=setting_seed)\n",
    "# selecting model shape\n",
    "data_shape = x_input.shape[1:]\n",
    "model = create_model()\n",
    "model_history = model.fit(x_train, y_train, epochs=15, batch_size=32, validation_split=0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"{top}_gadf_model.h5\")\n",
    "plot_model_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test data\n",
    "model.evaluate(x_test, y_test)\n",
    "y_predict = model.predict(x_test)\n",
    "y_predict=np.argmax(y_predict,axis=1)\n",
    "\n",
    "# confusion matrix\n",
    "cf_matrix = confusion_matrix(y_predict,y_test)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_predict,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data\n",
    "x_input = x_mtf_data\n",
    "\n",
    "# split in to test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, Y, test_size=0.30, random_state=setting_seed)\n",
    "# selecting model shape\n",
    "data_shape = x_input.shape[1:]\n",
    "model = create_model()\n",
    "model_history = model.fit(x_train, y_train, epochs=15, batch_size=32, validation_split=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"{top}_mtf_model.h5\")\n",
    "plot_model_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test data\n",
    "model.evaluate(x_test, y_test)\n",
    "y_predict = model.predict(x_test)\n",
    "y_predict=np.argmax(y_predict,axis=1)\n",
    "\n",
    "# confusion matrix\n",
    "cf_matrix = confusion_matrix(y_predict,y_test)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_predict,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data\n",
    "x_input = x_combined\n",
    "\n",
    "# split in to test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, Y, test_size=0.30, random_state=setting_seed)\n",
    "# selecting model shape\n",
    "data_shape = x_input.shape[1:]\n",
    "model = create_model()\n",
    "model_history = model.fit(x_train, y_train, epochs=15, batch_size=32, validation_split=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"{top}_combined_model.h5\")\n",
    "plot_model_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test data\n",
    "model.evaluate(x_test, y_test)\n",
    "y_predict = model.predict(x_test)\n",
    "y_predict=np.argmax(y_predict,axis=1)\n",
    "\n",
    "# confusion matrix\n",
    "cf_matrix = confusion_matrix(y_predict,y_test)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_predict,y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
